{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import ResNet18_Weights, EfficientNet_B0_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/'  # Substitua pelo caminho do seu diretório local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# train_dir = os.path.join(data_dir, 'train')\n",
    "# test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "train_dir = '../data/train/'\n",
    "test_dir = '../data/test/'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo pré-treinado ResNet-18 e modificar a última camada\n",
    "resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "num_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_features, len(train_dataset.classes))  # Ajustar para o número de classes do dataset\n",
    "\n",
    "# Mover o modelo para o dispositivo (GPU)\n",
    "resnet = resnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=101, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função de perda e otimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10, checkpoint_path='../model/checkpoints/checkpoint.pth'):\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Carregar checkpoint se disponível\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Checkpoint loaded. Starting from epoch {start_epoch}\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Mover dados para a GPU\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Time: {epoch_time:.2f}s')\n",
    "        \n",
    "        # Salvar checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss,\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Loss: 2.5964 Acc: 0.3859 Time: 179.78s\n",
      "Epoch 2/10 Loss: 1.6767 Acc: 0.5729 Time: 170.56s\n",
      "Epoch 3/10 Loss: 1.4276 Acc: 0.6289 Time: 172.91s\n",
      "Epoch 4/10 Loss: 1.2813 Acc: 0.6609 Time: 182.08s\n",
      "Epoch 5/10 Loss: 1.1745 Acc: 0.6889 Time: 176.56s\n",
      "Epoch 6/10 Loss: 1.0933 Acc: 0.7085 Time: 180.98s\n",
      "Epoch 7/10 Loss: 1.0171 Acc: 0.7265 Time: 171.26s\n",
      "Epoch 8/10 Loss: 0.9574 Acc: 0.7422 Time: 173.52s\n",
      "Epoch 9/10 Loss: 0.8983 Acc: 0.7548 Time: 170.02s\n",
      "Epoch 10/10 Loss: 0.8489 Acc: 0.7661 Time: 175.15s\n"
     ]
    }
   ],
   "source": [
    "resnet_trained = train_model(resnet, train_loader, criterion, optimizer, num_epochs=10, checkpoint_path='../model/checkpoints/checkpoint_resnet18.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_generate_classification_report(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Mover dados para a GPU\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Gerar classification report\n",
    "    report = classification_report(all_labels, all_preds, target_names=train_dataset.classes)\n",
    "    print(f'Classification Report:\\n{report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "              apple_pie       0.58      0.55      0.56       200\n",
      "         baby_back_ribs       0.69      0.68      0.68       200\n",
      "                baklava       0.74      0.77      0.75       200\n",
      "         beef_carpaccio       0.73      0.76      0.74       200\n",
      "           beef_tartare       0.64      0.66      0.65       200\n",
      "             beet_salad       0.58      0.60      0.59       200\n",
      "               beignets       0.76      0.83      0.79       200\n",
      "               bibimbap       0.82      0.83      0.82       200\n",
      "          bread_pudding       0.49      0.59      0.54       200\n",
      "      breakfast_burrito       0.72      0.47      0.57       200\n",
      "             bruschetta       0.69      0.52      0.59       200\n",
      "           caesar_salad       0.70      0.77      0.73       200\n",
      "                cannoli       0.76      0.79      0.78       200\n",
      "          caprese_salad       0.81      0.68      0.74       200\n",
      "            carrot_cake       0.82      0.60      0.69       200\n",
      "                ceviche       0.55      0.46      0.50       200\n",
      "           cheese_plate       0.70      0.73      0.72       200\n",
      "             cheesecake       0.56      0.55      0.55       200\n",
      "          chicken_curry       0.65      0.56      0.60       200\n",
      "     chicken_quesadilla       0.69      0.73      0.71       200\n",
      "          chicken_wings       0.66      0.76      0.70       200\n",
      "         chocolate_cake       0.58      0.64      0.61       200\n",
      "       chocolate_mousse       0.47      0.45      0.46       200\n",
      "                churros       0.77      0.81      0.79       200\n",
      "           clam_chowder       0.76      0.82      0.79       200\n",
      "          club_sandwich       0.62      0.83      0.71       200\n",
      "             crab_cakes       0.56      0.61      0.58       200\n",
      "           creme_brulee       0.70      0.84      0.77       200\n",
      "          croque_madame       0.83      0.79      0.81       200\n",
      "              cup_cakes       0.70      0.72      0.71       200\n",
      "           deviled_eggs       0.81      0.83      0.82       200\n",
      "                 donuts       0.74      0.58      0.65       200\n",
      "              dumplings       0.83      0.81      0.82       200\n",
      "                edamame       0.89      0.92      0.91       200\n",
      "          eggs_benedict       0.87      0.80      0.83       200\n",
      "              escargots       0.83      0.72      0.77       200\n",
      "                falafel       0.60      0.71      0.65       200\n",
      "           filet_mignon       0.58      0.62      0.60       200\n",
      "         fish_and_chips       0.71      0.85      0.77       200\n",
      "              foie_gras       0.51      0.41      0.46       200\n",
      "           french_fries       0.77      0.83      0.80       200\n",
      "      french_onion_soup       0.85      0.70      0.77       200\n",
      "           french_toast       0.66      0.56      0.61       200\n",
      "         fried_calamari       0.77      0.72      0.74       200\n",
      "             fried_rice       0.91      0.72      0.80       200\n",
      "          frozen_yogurt       0.67      0.85      0.75       200\n",
      "           garlic_bread       0.74      0.67      0.70       200\n",
      "                gnocchi       0.59      0.49      0.54       200\n",
      "            greek_salad       0.67      0.73      0.70       200\n",
      "grilled_cheese_sandwich       0.65      0.58      0.62       200\n",
      "         grilled_salmon       0.65      0.55      0.60       200\n",
      "              guacamole       0.61      0.82      0.70       200\n",
      "                  gyoza       0.79      0.69      0.73       200\n",
      "              hamburger       0.88      0.54      0.67       200\n",
      "      hot_and_sour_soup       0.80      0.92      0.86       200\n",
      "                hot_dog       0.73      0.69      0.71       200\n",
      "       huevos_rancheros       0.50      0.58      0.54       200\n",
      "                 hummus       0.62      0.55      0.58       200\n",
      "              ice_cream       0.67      0.58      0.62       200\n",
      "                lasagna       0.65      0.59      0.62       200\n",
      "         lobster_bisque       0.70      0.74      0.72       200\n",
      "  lobster_roll_sandwich       0.74      0.77      0.76       200\n",
      "    macaroni_and_cheese       0.76      0.69      0.73       200\n",
      "               macarons       0.80      0.88      0.84       200\n",
      "              miso_soup       0.89      0.85      0.87       200\n",
      "                mussels       0.84      0.86      0.85       200\n",
      "                 nachos       0.64      0.69      0.66       200\n",
      "               omelette       0.62      0.51      0.56       200\n",
      "            onion_rings       0.81      0.81      0.81       200\n",
      "                oysters       0.85      0.88      0.86       200\n",
      "               pad_thai       0.74      0.82      0.78       200\n",
      "                 paella       0.70      0.76      0.72       200\n",
      "               pancakes       0.72      0.71      0.72       200\n",
      "            panna_cotta       0.53      0.59      0.56       200\n",
      "            peking_duck       0.68      0.71      0.69       200\n",
      "                    pho       0.77      0.89      0.82       200\n",
      "                  pizza       0.70      0.77      0.73       200\n",
      "              pork_chop       0.54      0.40      0.46       200\n",
      "                poutine       0.83      0.77      0.80       200\n",
      "              prime_rib       0.58      0.73      0.65       200\n",
      "   pulled_pork_sandwich       0.56      0.78      0.65       200\n",
      "                  ramen       0.82      0.69      0.75       200\n",
      "                ravioli       0.52      0.51      0.52       200\n",
      "        red_velvet_cake       0.70      0.72      0.71       200\n",
      "                risotto       0.70      0.57      0.63       200\n",
      "                 samosa       0.61      0.79      0.69       200\n",
      "                sashimi       0.81      0.85      0.83       200\n",
      "               scallops       0.63      0.61      0.62       200\n",
      "          seaweed_salad       0.81      0.91      0.85       200\n",
      "       shrimp_and_grits       0.65      0.58      0.62       200\n",
      "    spaghetti_bolognese       0.79      0.83      0.81       200\n",
      "    spaghetti_carbonara       0.89      0.81      0.85       200\n",
      "           spring_rolls       0.75      0.73      0.74       200\n",
      "                  steak       0.48      0.35      0.41       200\n",
      "   strawberry_shortcake       0.66      0.65      0.65       200\n",
      "                  sushi       0.80      0.66      0.72       200\n",
      "                  tacos       0.57      0.62      0.59       200\n",
      "               takoyaki       0.77      0.82      0.80       200\n",
      "               tiramisu       0.66      0.69      0.67       200\n",
      "           tuna_tartare       0.52      0.60      0.56       200\n",
      "                waffles       0.75      0.73      0.74       200\n",
      "\n",
      "               accuracy                           0.70     20200\n",
      "              macro avg       0.70      0.70      0.69     20200\n",
      "           weighted avg       0.70      0.70      0.69     20200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_generate_classification_report(resnet_trained, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet_trained.state_dict(), '../model/resnet18_fine_tuned.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
